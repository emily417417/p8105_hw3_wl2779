---
title: "hw3_wl2779"
author: "wenyi Liu"
date: "10/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
```

## Problem 1
## some notes: 1,384,617  size, 

```{r data input & look at sisle}
library(p8105.datasets)
data("instacart")

aisle_number=count(instacart,aisle)
arrange(aisle_number,desc(n))
```
There are #134 aisles and ["Fresh vegetables"] aisles are the most items ordered from.


```{r plots}
aisle_plots=
  aisle_number %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point(aes(color = aisle)) +
  labs(
    title = "Aisle plot",
    x = "The name of the aisle",
    y = "The number of items ordered in each aisle",
    caption = "Data from the instacart"
   )
  aisle_plots
```
Above is the plots that shows the the number of items ordered in each aisle.

```{r table of 3 most popular items_data cleaning}
aisle_table_popular=
  select(instacart,aisle, product_name)%>%
  filter( aisle=="baking ingredients" |aisle=="dog food care"| aisle=="packaged vegetables fruits" ) %>%
  group_by(aisle,product_name) %>%
  summarize(n_obs = n()) %>%
  top_n(3)%>%
  arrange(desc(n_obs))
```

```{r table of 3 most popular items}
knitr::kable(aisle_table_popular,
             caption="Top 3 most popular items by Aisle groups",
             format="simple",
             col.name=str_to_title(names(aisle_table_popular)))
```
Above is the table that showing the Top 3 most popular items by selected 3 Aisle groups.

```{r table of mean hour_data cleaning}
mean_hour=
  select(instacart,order_dow, order_hour_of_day,product_name)%>%
  filter( product_name=="Pink Lady Apples" | product_name=="Coffee Ice Cream") %>%
  group_by(product_name,order_dow) %>%
  summarize(mean = mean(order_hour_of_day)) 
```

```{r}
mean_hour
```

## need to make table!!!!!!!!!

```{r human_read_table}
mean_hour_coffee=
  mean_hour%>%
  filter( product_name=="Coffee Ice Cream" )%>%
  mutate(
    mm= recode(order_dow, 
    "Sunday"="0",
    "Monday"="1",
    "Tuesday"="2",
    "Wednesday"="3",
    "Thursday"="4",
    "Friday"="5",
    "Saturday"="6")
  )%>%
  select(mm,product_name)
```


##Description in Problem1 dataset##

!!!!!!!!!!!!!!!!!!!!

## Problem 2

```{r data input}
library(p8105.datasets)
data("brfss_smart2010")
```

```{r data_cleaning}
brfss=
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic=="Overall Health") %>%
  filter(response=="Excellent"|response=="Poor") %>%
  arrange(desc(response)) %>%
  rename(state=locationabbr) %>%
  rename(location=locationdesc)
```

Now, the data is well cleaned.

```{r q1}
##2010
brfss_q1_2010=
  brfss %>%
  group_by(year,state) %>%
  summarise(number=n_distinct(location))%>%
  filter(year==2010)%>%
  filter(number>=7) %>%
  arrange(year,state,number)

## 2002
brfss_q1_2002=
  brfss %>%
  group_by(year,state) %>%
  summarise(number=n_distinct(location))%>%
  filter(year==2002)%>%
  filter(number>=7) %>%
  arrange(year,state,number)
```

```{r q1 summary}
brfss_q1_2010
brfss_q1_2002
```

In 2002,CT,FL,MA,NC,NJ,PA were observed at 7 or more locations.
In 2010,CA,CO,FL,MA,MD,NC,NE,NJ,NY,OH,PA,SC,TX,WA were observed at 7 or more locations.

```{r q2 data cleaning}
brfss_q2=
  brfss %>%
  filter(response=="Excellent") %>%
  group_by(state,year)%>%
  summarize(average=mean(data_value))%>%
  arrange(state,year,average)
```

```{r q2 plot}
ggplot(data = brfss_q2, aes(x = year, y = average, color = state)) + 
  geom_point() + 
  geom_line(data = brfss_q2)+
   labs(
    title = " The average value over time within a state ",
    x = "Year",
    y = "The average data value across locations within a state",
    caption = "Data from BRFSS"
   )
```

From the gglots output, we can know that for each state, the average data value across locations are fluctuate over time from 2002 to 2010.  

```{r q3 data cleaning for 2006}
brfss_q3=
  brfss %>%
  filter(state=="NY")%>%
  filter(year==2006|year==2010)%>%
  group_by(response)%>%
  arrange(desc(response)) %>%
  select(year,state,response,data_value)
```

```{r}
brfss_q3
```

```{r}
ggplot(data = brfss_q3, aes(x = response, y = data_value, color=response)) + 
  geom_point() +
   labs(
    title = " The distribution of data_value for responses among locations in NY State ",
    x = "Response",
    y = "data_value",
    caption = "Data from BRFSS"
   )+
  facet_grid(~year)
```

From the two pane plots, we can know that in both 2006 and 2010, the data_value in excellent response group is distributed in higher value than the Poor response group..


## Problem 3

```{r data import}
accel_data= 
  read_csv(
    "./data/accel_data.csv",show_col_types = FALSE) %>%
   janitor::clean_names() %>%
   mutate(weektype= case_when(
    day =="Saturday"  ~ "Weekend",
    day =="Sunday"  ~ "Weekend",
    day != "Saturday" ~ "Weekday",
    day != "Sunday" ~ "Weekday",
    TRUE     ~ ""
  )) 
```

```{r look at data}
accel_data
```
Description: There are 35 rows and 14,444 columns. The first 3 columns are week number#, the unique da id and the data(from Monday to Friday). The colums from the No.4 to the end are activity time records. 


```{r q1_data_cleaning}
accel_data_q1=
  accel_data %>%
  group_by(week,day) %>%
  summarize(sum_data=sum(activity_1:activity_1440))
```


```{r q1_table}
knitr::kable(accel_data_q1,
             caption="total activity variable for each day",
             format="markdown",digits = 0,
             col.name=str_to_title(names(accel_data_q1)))
```
Above is the table it let me know that the first two weeks have larger sum of total activity variables.

```{r q2_plot}
ggplot(data = accel_data_q1, aes(x = week, y = sum_data, color=day)) + 
  geom_point() +
   labs(
    title = " Single-panel plot that shows the 24-hour activity time courses for each day ",
    x = "week number#",
    y = "sum activity time",
    caption = "Data from Accelerometers ")+
    scale_y_continuous(
    breaks = c(0, 1000, 10000,100000,200000,300000,400000),
    labels = c("0", "1,000", "10,000","100,000","200,000,","300,000","400,000")
   )
```
From the plot output, sum activity time in Wednesday usually keeps around ~1000 except in Week4. Week2 and Week4 has an day in which the total activity time is quite large.


